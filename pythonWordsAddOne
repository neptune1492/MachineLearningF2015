import math
import re
files = ['00', '01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12', '13', '14']

#open word freq files for dialects
c_data = open('c_word_freq', 'r')
m_data = open('m_word_freq', 'r')
u_data = open('u_word_freq', 'r')

#initialize word counts to 0
cTotWords = 0
mTotWords = 0
uTotWords = 0
vocabC = 0
vocabM = 0
vocabU = 0

#get wordcount by adding-up freq column
for cline in c_data:
	lineParts = cline.split()
	cTotWords += int(lineParts[0])
	vocabC +=1
#	print cTotWords
for mline in m_data:
	lineParts = mline.split()
	mTotWords += int(lineParts[0])
	vocabM +=1
#	print mTotWords
for uline in u_data:
	lineParts = uline.split()
	uTotWords += int(lineParts[0])
	vocabU +=1
alpha = 1


#	print uTotWords
#loop through test files
for t in range(0,15): #which xx[] files to read
	print ['Iteration', t]
	
	#open the next tweet
	test_tweet = 'xx' + files[t]
	test_data = open(test_tweet, 'r')#open the test data file


	textProbC = 0 
	textProbM = 0
	textProbU = 0
	
	
	probC = 0.38 #calculated by hand - 19/50 training tweets are from C
	probM = 0.14 #7/50 tweets from M
	probU = 0.48 #24/50 tweets from U

	

	#go through each line of tweets file
	
	for tline in test_data: #for every line of tweet
		w=re.sub("[,.!]", " ", tline) #replace punctuation with space
			
		w=w.split() #split on whitespace
		
		for i in range(0, len(w)):
		#for i in range(0,1): #which words to pull out.
			word = w[i].lower()	#get a new word
#			print ['word', word]
			switch =1
			#initialize probability for new word to 0	
			p_wordC = 0 
			p_wordM = 0
			p_wordU = 0

			
			for line in m_data:
				mtext = re.sub("[.,!]", " ", line)
				parts = mtext.split()
				if parts[1] == word:
					p_wordM = float(parts[0]+alpha)/(mTotWords +(alpha* vocabM))
					switch = 0
				if switch == 0:
					break
			if switch == 1:
				p_wordM = float(alpha)/(mTotWords + (alpha*vocabM))
			
			textProbM += math.log(p_wordM, 2)
			


			switch = 1
			#check curr word in tweet against every line of c_data
			mf = open('c_word_freq', 'r')
			for line in mf:
				ctext = re.sub("[.,!]", " ", line)
				parts = ctext.split() #splitspace, into columns
				if parts[1] == word:
					p_wordC = (float(parts[0])+alpha)/(cTotWords+(alpha * vocabC))
					switch = 0
				if switch == 0:
					break #break out and move on
			#after the c data has been checked, see if not found:
			if switch == 1: #if p hasn't been assigned yet
			#assign a small probability
				p_wordC = float(alpha)/(cTotWords + (alpha *vocabC))
				#print 'word not found:'
				#print word
			textProbC += math.log(p_wordC, 2)
			

			switch = 1	

			#repeat for U
			switch = 1
			u_data = open('u_word_freq', 'r')
			for line in u_data:
				utext = re.sub("[,.!]", " ", line)
				partsU = utext.split()
				#print partsU[1] 
				#print word
				if partsU[1] == word:
					#match
					switch = 0
					#print 'U'
					#print float(partsU[0])
					#print uTotWords
					
				#	print ['FOUND u  word', partsU[1]]
					p_wordU = (float(partsU[0])+alpha)/(uTotWords + (alpha*vocabU))
	#				print p_wordU
	#				
				if switch ==0:
					break
			if switch == 1:
				p_wordU =float(alpha)/(uTotWords + (alpha * vocabU))
			textProbU += math.log(p_wordU, 2)
			#	print ['assigning 0.001', word]
			#print ['p_wordU',  p_wordU]
			
		#	print ['textProbU', textProbU]
	
		#	print 'c'
 		#	print textProbC
		#	print 'm'
		#	print textProbM
		#	print 'u'
		#	print textProbU	
		#end for i in len(w)
	
	#then at the end of the xx[] file, we add in log(P(class))
	textProbC += math.log(probC, 2)
	textProbM += math.log(probM, 2)
	textProbU += math.log(probU, 2)
	
	list = [textProbC, textProbM, textProbU]
	if max(list) == list[0]:
		print 'C'
	if max(list) == list[1]:
		print 'M'
	if max(list) == list[2]:
		print 'U'

#	print list
